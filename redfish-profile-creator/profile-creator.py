# DMTF 2025

import sys
import json
import csv
import argparse
import logging
import copy
from collections.abc import Mapping

# Create Logger
my_logger = logging.getLogger(__name__)
my_logger.setLevel(logging.DEBUG)

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
my_logger.addHandler(handler)

LOGGING_VERBOSE = logging.INFO - 1

logging.addLevelName(LOGGING_VERBOSE, "VERBOSE")

TOOL_VERSION = 0.1

# Pulled from SampleProfile.json
HEADER_PROFILE = {
    "SchemaDefinition": "RedfishInteroperabilityProfile.v1_8_0",
    "ProfileName": "Default Profile",
    "ProfileVersion": "1.0.0",
    "Purpose": "This is a default Redfish Interoperability profile.",
    "OwningEntity": "Default",
    "ContactInfo": "default@nothing(dot)com",
    "Protocol": {
        "MinVersion": "1.6",
        "Discovery": "Recommended",
        "HostInterface": "Mandatory",
        "ExpandQuery": "Recommended",
        "SelectQuery": "None",
        "FilterQuery": "Recommended"
    },
    "Resources": {}
}

HEADER_RESOURCE = {
    "PropertyRequirements": {}
}

HEADER_RESOURCE_EXTRA = {
    "MinVersion": "1.0.0",
    "Purpose": "Autogenerated",
    "ReadRequirement": "Mandatory",
    "CreateResource": True,
    "DeleteResource": False,
    "UpdateResource": True
}

HEADER_CONDITIONAL = {
        "ConditionalRequirements": [{
                "CompareProperty": 'PropertyName',
                "CompareType": "Equal",
                "CompareValues": ['PropertyValue'],
                "URIs": ['/path/to/resource'],
                "SubordinateToResource": ['Type1', 'Type2'],
                "Purpose": "Automatically generated Condition.  If a condition above is True, apply the following interop below.",
                "ReadRequirement": "Recommended",
                "WriteRequirement": "Recommended",
                "Comparison": "Equal",
                "Values": ["Value1", "Value2", "Value3"],
                "PropertyRequirements": {}
        }]
}

HEADER_USECASE = {
    "UseCaseTitle": "Default",
    "UseCaseType": "Normal",
    "Purpose": "Automatically generated UseCase, using the following criteria",
    "URIs": ['/path/to/resource'],
    "SubordinateToResource": ['Type1', 'Type2'],
    "UseCaseKeyProperty": "TargetProperty",
    "UseCaseKeyValues": ["SomeValue"],
    "UseCaseComparison": "Equal"
}

HEADER_ACTION = {
    "ActionInfo": "Recommended",
    "Parameters": {}
}

HEADER_ACTION_PARAMETER = {
    "ParameterValues": ["Value1", "Value2", "Value3"],
    "RecommendedValues": ["Value4", "Value5", "Value6"],
    "ReadRequirement": "Mandatory"
}

# List CSV header values and their possible values
CSV_FIELDS = {'Schema': str,
                'UseCase': str,
                'Property': str,
                'ReadRequirement': ["", "Mandatory", "Supported", "Recommended", "IfImplemented", "IfPopulated", "Conditional", "None"],
                'WriteRequirement': ["", "Mandatory", "Supported", "Recommended", "None"],
                'Conditional': bool,
                'Purpose': str
               }
        
CSV_LOWER_TO_CAMEL = {
    x: y for x, y in zip([n.lower() for n in CSV_FIELDS['ReadRequirement']], CSV_FIELDS['ReadRequirement'])
}
                

DEFAULT_FIELDS = ['','','','','','False','None']

AUTOGENERATED_GROUPS = ['UseCases']

AUTOGENERATED_FIELDS = ["ConditionalRequirements", "ActionInfo", "ParameterValues", "RecommendedValues", "MinVersion"]

def dict_merge(dct, merge_dct, merge_behavior="default", current_path=None):
    """
    https://gist.github.com/angstwad/bf22d1822c38a92ec0a9 modified
    Recursive dict merge. Inspired by :meth:``dict.update()``, instead of
    updating only top-level keys, dict_merge recurses down into dicts nested
    to an arbitrary depth, updating keys. The ``merge_dct`` is merged into
    ``dct``.
    :param dct: dict onto which the merge is executed
    :param merge_dct: dct merged into dct
    :return: None
    """
    if not current_path:
        current_path = []
    for k in merge_dct:
        new_path = ".".join(current_path + [k])
        my_logger.log(LOGGING_VERBOSE - 1, new_path)
        if (k in dct and isinstance(dct[k], dict) and isinstance(merge_dct[k], Mapping)):
            if k not in AUTOGENERATED_GROUPS or merge_behavior == "default":
                dict_merge(dct[k], merge_dct[k], merge_behavior, current_path + [k]) 
            else:
                my_logger.info("Skipping autogenerated group {}".format(new_path))
        elif k in dct:
            if merge_behavior == "default":
                dct[k] = merge_dct[k]
            elif k in AUTOGENERATED_FIELDS and merge_behavior != "always":
                my_logger.info("Skipping autogenerated field {}".format(new_path))
            elif merge_behavior.lower() in ["prefer_old", "skip"]:
                my_logger.warning("Skipping change to {}".format(new_path))
            elif merge_behavior.lower() == "prefer_new":
                my_logger.warning("Overwriting {}".format(new_path))
                dct[k] = merge_dct[k]
            else:
                dct[k] = merge_dct[k]
        else:
            if merge_behavior.lower() == "skip" and not isinstance(merge_dct[k], dict):
                my_logger.warning("Skipping addition of {}".format(new_path))
            else:
                my_logger.log(LOGGING_VERBOSE, "Adding {}".format(new_path))
                dct[k] = merge_dct[k]


def is_empty(value : str):
    return value is None or value.lower().strip(' ') in ['', 'null', 'n/a']


def parse_csv_line(entry: list):
    """Given an array from a CSV file parser, parse according to FIELDS

    Returns a dictionary object

    Args:
        line (str): CSV Row
    """    

    my_logger.log(LOGGING_VERBOSE, entry)

    # Verify and Format CSV values for processing 
    if len(entry) < 3:
        raise ValueError("CSV entry needs at least 3 values")
    
    entry_values = entry[0:len(entry)] + DEFAULT_FIELDS[len(entry):]

    entry_dict = {x: y.strip() for x, y in zip(CSV_FIELDS.keys(), entry_values)}

    if any([is_empty(entry_dict[k]) for k in ['Schema', 'Property']]):
        raise ValueError("Values Schema and Property cannot be None")

    entry_dict['ReadRequirement'] = CSV_LOWER_TO_CAMEL.get(entry_dict['ReadRequirement'], entry_dict['ReadRequirement'])
    if entry_dict['ReadRequirement'] not in CSV_FIELDS['ReadRequirement']:
        raise ValueError("ReadRequirement not a valid value: {}".format(CSV_FIELDS['ReadRequirement']))

    entry_dict['WriteRequirement'] = CSV_LOWER_TO_CAMEL.get(entry_dict['WriteRequirement'], entry_dict['WriteRequirement'])
    if entry_dict['WriteRequirement'] not in CSV_FIELDS['WriteRequirement']:
        raise ValueError("WriteRequirement not a valid value: {}".format(CSV_FIELDS['WriteRequirement']))
    
    if entry_dict['Conditional'].lower().strip() in ['false','no','n'] or is_empty(entry_dict['Conditional']):
        entry_dict['Conditional'] = False
    elif entry_dict['Conditional'].lower().strip() in ['true','yes','y']:
        entry_dict['Conditional'] = True
    else:
        raise ValueError("Conditional not a bool: {}".format(entry_dict['Conditional']))
    
    # Process CSV
    my_schema = entry_dict['Schema']
    my_usecase = entry_dict['UseCase']
    needs_conditional = entry_dict['Conditional']

    # When creating UseCases, use a dict.
    # After merging the dict in postprocessing, convert dict to proper LIST
    if not is_empty(my_usecase):
        section = { my_schema: {} }
        section[my_schema]['UseCases'] = {my_usecase: copy.deepcopy(HEADER_USECASE)}
        section[my_schema]['UseCases'][my_usecase]['UseCaseTitle'] = my_usecase
        working_section = section[my_schema]['UseCases'][my_usecase]
    else:
        section = { my_schema: copy.deepcopy(HEADER_RESOURCE) }
        working_section = section[my_schema]

    property_chain = entry_dict['Property'].split('.')

    if property_chain[0] == 'Actions' and len(property_chain) > 1:
        action_name = property_chain[1]
        working_section['ActionRequirements'] = {action_name: copy.deepcopy(HEADER_ACTION)}
        if len(property_chain) == 2:
            for k in ['ReadRequirement', 'Purpose']:
                if not is_empty(entry_dict[k]):
                    working_section['ActionRequirements'][action_name][k] = entry_dict[k]
        if len(property_chain) == 3:
            param_name = property_chain[2]
            working_section['ActionRequirements'][action_name]['Parameters'] = {param_name: copy.deepcopy(HEADER_ACTION_PARAMETER)}
            for k in ['ReadRequirement']:
                if not is_empty(entry_dict[k]):
                    working_section['ActionRequirements'][action_name]['Parameters'][param_name][k] = entry_dict[k]

    else:
        for prop_name in property_chain:
            if 'PropertyRequirements' not in working_section:
                working_section['PropertyRequirements'] = {}
            working_section['PropertyRequirements'][prop_name] = {}
            working_section = working_section['PropertyRequirements'][prop_name]
            my_logger.log(LOGGING_VERBOSE - 1, prop_name)

        for k in ['ReadRequirement', 'WriteRequirement', 'Purpose']:
            if not is_empty(entry_dict[k]):
                working_section[k] = entry_dict[k]
        if needs_conditional:
            working_section["ConditionalRequirements"] = copy.deepcopy(HEADER_CONDITIONAL)


    my_logger.log(LOGGING_VERBOSE - 1, section)

    return section


def convert_usecases_to_dict(profile):
    my_resources = profile.get('Resources', {})
    for name, res in my_resources.items():
        if 'UseCases' in res:
            case_list = res['UseCases']
            case_dict = {case['UseCaseTitle']: case for case in case_list}
            res['UseCases'] = case_dict


def convert_usecases_to_list(profile):
    my_resources = profile.get('Resources', {})
    for name, res in my_resources.items():
        if 'UseCases' in res:
            case_dict = res['UseCases']
            case_list = [case for case_name, case in case_dict.items()]
            res['UseCases'] = case_list


def main(argslist=None, configfile=None):
    """Main command

    Args:
        argslist ([type], optional): List of arguments in the form of argv. Defaults to None.
    """
    argget = argparse.ArgumentParser(description='DMTF Tool to generate Profiles from CSV files {}'.format(TOOL_VERSION))

    # base tool
    argget.add_argument('csv_file', help='CSV File')
    argget.add_argument('-p', '--profile', type=str, help='Profile to use as a base for creation, default None')
    argget.add_argument('-b', '--behavior', default="profile", type=str, help='Behavior for overwriting [profile, csv, skip], Default: Profile')
    argget.add_argument('-o', '--output', type=str, help='Output file, will overwrite target')

    argget.add_argument('-vvv', '--verbose', action='count', default=0, help='Verbosity of tool in stdout')

    # parse...
    args = argget.parse_args(argslist)

    handler.setLevel(logging.INFO - args.verbose)

    my_logger.info("Using CSV file: {}".format(args.csv_file))

    if args.output is None:
        args.output = args.csv_file + ".output.json"

    if args.profile:
        with open(args.profile) as f:
            input_profile = json.load(f)
            convert_usecases_to_dict(input_profile)
            # my_logger.info(json.dumps(input_profile, indent=2))
            
    else:
        input_profile = copy.deepcopy(HEADER_PROFILE)
    
    if args.behavior.lower() not in ['profile', 'csv', 'skip']:
        my_logger.error("Overwrite behavior should be in [Profile, CSV, Skip]")
        return 1

    with open(args.csv_file, newline='') as f:
        my_csv = csv.reader(f)
        generated_profile = {"Resources": {}}
        new_sections = generated_profile["Resources"]
        for line in my_csv:
            new_section = parse_csv_line(line)
            dict_merge(new_sections, new_section)

        convert_usecases_to_list(generated_profile)
        my_logger.log(LOGGING_VERBOSE, json.dumps(new_sections, indent=2))
        
        if args.profile:
            if args.behavior == 'profile':
                dict_merge(input_profile, generated_profile, "prefer_old")
            if args.behavior == 'csv':
                dict_merge(input_profile, generated_profile, "prefer_new")
            if args.behavior == 'skip':
                dict_merge(input_profile, generated_profile, "skip")
        else:
            dict_merge(input_profile, generated_profile)
        my_logger.log(LOGGING_VERBOSE - 1,json.dumps(input_profile, indent=2))

    with open(args.output, mode='w') as f:
        my_logger.info("Writing to file {}".format(args.output))
        json.dump(input_profile, f, indent=2)

    return 0

    

if __name__ == '__main__':
    try:
        sys.exit(main())
    except Exception as e:
        my_logger.exception("Program finished prematurely: %s", e)
        raise